# Import required packages.
import streamlit as st
import json
from pydantic import BaseModel
import openai
client = openai.OpenAI(api_key=st.secrets["OpenAI_key"])

# Backend

#Creates the prompt for ChatGPT.
#
#   Args:
#        content: the content that needs metadata.
#    Returns:
#        A structured prompt for OpenAI.    

def prompt(content):
    return [
        {'role': 'system', 
         'content': f"""Your our a course author given the task of breaking down learning content into clearly defined metadata. 
         				When someone gives you content, whether that is a course concept, video transcript, or a piece of a lesson, 
         				you job is to create the relevant learning data that describes that content object."""},
        {'role': 'user', 
         'content': f"""Define the metadata of this content: {content}"""}
            ]


#A class constraint that is passed to OpenAI.

class metadataStructure(BaseModel):
    subject: str 
    difficulty: str 
    skills: list[str]
    language: str 
    learningObjectives: list[str] 
    targetAudience: list[str]
    tone: list[str]
    prerequisite: str 

#Handles calling OpenAI.
#
#    Args:
#        message: The prompt generated by `prompt`.
#        model: The model being used (in this case, a specific snapshot of 4o that supports structured outputs.
#    
#    Returns:
#        OpenAI's results.

def chatgpt(message,model='gpt-4o-2024-08-06'):
    while True:
        try:
            result = client.beta.chat.completions.parse(
                      model=model,
                      messages=message,
                      response_format=metadataStructure)
            
            content = result.choices[0].message.parsed
            return content
        
        # Lazy exception error handling to back off and try the API again.
        except Exception as e:
            print(e)
            time.sleep(5)

# Frontend

# Title
st.title('Demo: Content Metadata Generation Using Structured Outputs')

# Input box for pasting content.
st.markdown('### Input')
sampleInput = """[Opening Scene: Introduction] "Welcome to today's lesson on the data lifecycle, a crucial concept for every data product manager to master. In this session, we'll explore how data journeys from its origins in data generators, through data lakes, and eventually to the frontend consumption tools in the Business Intelligence, or BI, layer." [Scene 1: Data Generators] "Let's start with data generators. These are the sources where data is created—whether from user interactions on a platform, sensors in IoT devices, or transactional systems like e-commerce platforms. As a data product manager, you need to understand where and how data is generated because this will influence its structure, quality, and potential use cases." [Scene 2: Ingestion into Data Lakes] "Once generated, the data needs to be ingested into a central repository, often a data lake. Data lakes are scalable storage solutions that allow for the collection of raw, unstructured, and semi-structured data. Here, data is stored in its native format, providing flexibility for various types of data to coexist—whether it’s JSON, XML, or even video files. The key here is that data lakes provide the foundation for future analysis and processing, but they also require careful governance to avoid becoming disorganized 'data swamps.'" Scene 3: Transformation and Processing] "Before data can be useful, it often needs to be transformed—cleaned, aggregated, or enriched. This process usually takes place in a data warehouse or processing layer, where data is structured and optimized for specific queries. The transformation phase is critical, as it prepares data for consumption in a way that aligns with business needs." [Scene 4: Consumption in the BI Layer] "Finally, we arrive at the BI layer—the tools and platforms where end-users, often business analysts or executives, interact with the data. Here, data is visualized and analyzed, turning raw information into actionable insights. Dashboards, reports, and real-time analytics are all part of this layer, helping decision-makers drive strategy and operations based on data-driven insights." Closing Scene: Summary] "In summary, understanding the data lifecycle—from generation to consumption—is vital for data product managers. It allows you to design better products, ensure data quality, and ultimately drive more value for your organization. Thanks for joining, and see you in the next lesson!"
"""
content = st.text_area('Input content and click "Generate Summary"',sampleInput, height=300)

# Button to trigger job.
if st.button('Generate Summary'):

	# Processing task.
	with st.spinner('Generating structured outputs...'):

		# Generate output.
		response = chatgpt(message=prompt(content))

		# Unpack data.
		data = {
			'subject': response.subject,
			'difficulty': response.difficulty,
			'skills': response.skills,
			'language': response.language,
			'learningObjectives': response.learningObjectives,
			'targetAudience': response.targetAudience,
			'tone': response.tone,
			'prerequisite':response.prerequisite
			}

		# Pretty formatting of results for output.
		json_formatted_str = json.dumps(data, indent=4)

		# Render output that was given.
		st.markdown('### Output')
		st.code(json_formatted_str,language="json")
